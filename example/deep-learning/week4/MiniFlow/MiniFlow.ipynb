{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You need to change the Add() class below.\n",
    "\"\"\"\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # an Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node that may\n",
    "    # receive its value as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should calculate their\n",
    "    # values from the value of previous nodes, using\n",
    "    # self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            \n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost * 1\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        x_value = self.inbound_nodes[0].value\n",
    "        y_value = self.inbound_nodes[1].value\n",
    "        self.value = x_value + y_value\n",
    "\n",
    "# class Linear(Node):\n",
    "#     def __init__(self, inputs, weights, bias):\n",
    "#         Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "#         # NOTE: The weights and bias properties here are not\n",
    "#         # numbers, but rather references to other nodes.\n",
    "#         # The weight and bias values are stored within the\n",
    "#         # respective nodes.\n",
    "\n",
    "#     def forward(self):\n",
    "#         inputs = self.inbound_nodes[0].value\n",
    "#         weights = self.inbound_nodes[1].value\n",
    "#         bias = self.inbound_nodes[2]\n",
    "#         self.value = bias.value\n",
    "#         for x, w in zip(inputs, weights):\n",
    "#             self.value += x * w\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the linear transform output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "No need to change anything below here!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5 = 15 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script builds and runs a graph with miniflow.\n",
    "\n",
    "There is no need to change anything to solve this quiz!\n",
    "\n",
    "However, feel free to play with the network! Can you also\n",
    "build a network that solves the equation below?\n",
    "\n",
    "(x + y) + y\n",
    "\"\"\"\n",
    "\n",
    "# from miniflow import *\n",
    "\n",
    "x, y = Input(), Input()\n",
    "\n",
    "f = Add(x, y)\n",
    "\n",
    "feed_dict = {x: 10, y: 5}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The setup is similar to the prevous `Linear` node you wrote\n",
    "except you're now using NumPy arrays instead of python lists.\n",
    "\n",
    "Update the Linear class in miniflow.py to work with\n",
    "numpy vectors (arrays) and matrices.\n",
    "\n",
    "Test your code here!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "# output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sigmoid Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "    \n",
    "    def _sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        return (self._sigmoid(x) * ( 1 - self._sigmoid(x) ))\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        # This is a dummy value to prevent numpy errors\n",
    "        # if you test without changing this method.\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "#             self._sigmoid_derivative(grad_cost)\n",
    "            \n",
    "            self.gradients[self.inbound_nodes[0]] += self._sigmoid_derivative(self.inbound_nodes[0].value) * grad_cost\n",
    "\n",
    "#             sigmoid = self.value\n",
    "#             self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "            \"\"\"\n",
    "            TODO: Your code goes here!\n",
    "\n",
    "            Set the gradients property to the gradients with respect to each input.\n",
    "\n",
    "            NOTE: See the Linear node and MSE node for examples.\n",
    "            \"\"\"\n",
    "        \n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "# output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "# # 2 by 2 matrices\n",
    "# w1  = np.array([[1, 2], [3, 4]])\n",
    "# w2  = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# print(w1)\n",
    "# print(w2)\n",
    "\n",
    "# # flatten\n",
    "# w1_flat = np.reshape(w1, -1)\n",
    "# w2_flat = np.reshape(w2, -1)\n",
    "\n",
    "# print(w1_flat)\n",
    "# print(w2_flat)\n",
    "\n",
    "\n",
    "# w = np.concatenate((w1_flat, w2_flat))\n",
    "# # array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# print(w)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "#         print(\"y before reshape\")\n",
    "        y = self.inbound_nodes[0].value\n",
    "#         print(y)\n",
    "#         print(\"y after\")\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "#         print(y)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        \n",
    "#         print(\"y-a\")\n",
    "#         print(y-a)\n",
    "        \n",
    "        y1 = self.inbound_nodes[0].value\n",
    "        a1 = self.inbound_nodes[1].value\n",
    "        \n",
    "#         print(\"y1-a1\")\n",
    "#         print(y1-a1)\n",
    "\n",
    "#         self.value = np.sum(np.square(a-y)) / (a-y).shape[0]\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "\n",
    "        This is the final node of the network so outbound nodes\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "def forward_pass(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        \n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]]\n",
      "gradients\n",
      "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test your network here!\n",
    "\n",
    "No need to change this code, but feel free to tweak it\n",
    "to test your network!\n",
    "\n",
    "Make your changes to backward method of the Sigmoid class in miniflow.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(\"gradients\")\n",
    "print(gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Change the trainable's value by subtracting the learning rate\n",
    "    # multiplied by the partial of the cost with respect to this\n",
    "    # trainable.\n",
    "    for t in trainables:\n",
    "#         print(\"t.gradients\")\n",
    "#         print(t.gradients)\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 126.844\n",
      "Epoch: 2, Loss: 37.969\n",
      "Epoch: 3, Loss: 21.598\n",
      "Epoch: 4, Loss: 22.122\n",
      "Epoch: 5, Loss: 18.695\n",
      "Epoch: 6, Loss: 22.142\n",
      "Epoch: 7, Loss: 21.323\n",
      "Epoch: 8, Loss: 23.240\n",
      "Epoch: 9, Loss: 18.815\n",
      "Epoch: 10, Loss: 13.037\n",
      "Epoch: 11, Loss: 18.276\n",
      "Epoch: 12, Loss: 16.008\n",
      "Epoch: 13, Loss: 14.196\n",
      "Epoch: 14, Loss: 13.228\n",
      "Epoch: 15, Loss: 11.891\n",
      "Epoch: 16, Loss: 14.365\n",
      "Epoch: 17, Loss: 12.267\n",
      "Epoch: 18, Loss: 10.867\n",
      "Epoch: 19, Loss: 14.977\n",
      "Epoch: 20, Loss: 12.616\n",
      "Epoch: 21, Loss: 11.028\n",
      "Epoch: 22, Loss: 8.705\n",
      "Epoch: 23, Loss: 11.186\n",
      "Epoch: 24, Loss: 10.832\n",
      "Epoch: 25, Loss: 9.269\n",
      "Epoch: 26, Loss: 9.103\n",
      "Epoch: 27, Loss: 9.774\n",
      "Epoch: 28, Loss: 8.809\n",
      "Epoch: 29, Loss: 8.043\n",
      "Epoch: 30, Loss: 9.136\n",
      "Epoch: 31, Loss: 7.508\n",
      "Epoch: 32, Loss: 7.364\n",
      "Epoch: 33, Loss: 8.147\n",
      "Epoch: 34, Loss: 10.178\n",
      "Epoch: 35, Loss: 7.725\n",
      "Epoch: 36, Loss: 9.354\n",
      "Epoch: 37, Loss: 8.470\n",
      "Epoch: 38, Loss: 7.407\n",
      "Epoch: 39, Loss: 8.060\n",
      "Epoch: 40, Loss: 6.997\n",
      "Epoch: 41, Loss: 8.278\n",
      "Epoch: 42, Loss: 8.747\n",
      "Epoch: 43, Loss: 6.271\n",
      "Epoch: 44, Loss: 8.780\n",
      "Epoch: 45, Loss: 8.454\n",
      "Epoch: 46, Loss: 7.722\n",
      "Epoch: 47, Loss: 7.611\n",
      "Epoch: 48, Loss: 6.107\n",
      "Epoch: 49, Loss: 7.569\n",
      "Epoch: 50, Loss: 9.468\n",
      "Epoch: 51, Loss: 8.164\n",
      "Epoch: 52, Loss: 8.675\n",
      "Epoch: 53, Loss: 7.268\n",
      "Epoch: 54, Loss: 6.242\n",
      "Epoch: 55, Loss: 6.816\n",
      "Epoch: 56, Loss: 6.313\n",
      "Epoch: 57, Loss: 7.446\n",
      "Epoch: 58, Loss: 5.608\n",
      "Epoch: 59, Loss: 6.810\n",
      "Epoch: 60, Loss: 6.912\n",
      "Epoch: 61, Loss: 6.211\n",
      "Epoch: 62, Loss: 6.457\n",
      "Epoch: 63, Loss: 5.997\n",
      "Epoch: 64, Loss: 6.458\n",
      "Epoch: 65, Loss: 7.452\n",
      "Epoch: 66, Loss: 6.516\n",
      "Epoch: 67, Loss: 7.920\n",
      "Epoch: 68, Loss: 5.752\n",
      "Epoch: 69, Loss: 5.750\n",
      "Epoch: 70, Loss: 6.683\n",
      "Epoch: 71, Loss: 6.282\n",
      "Epoch: 72, Loss: 6.394\n",
      "Epoch: 73, Loss: 6.028\n",
      "Epoch: 74, Loss: 6.047\n",
      "Epoch: 75, Loss: 6.360\n",
      "Epoch: 76, Loss: 7.681\n",
      "Epoch: 77, Loss: 6.059\n",
      "Epoch: 78, Loss: 6.626\n",
      "Epoch: 79, Loss: 6.255\n",
      "Epoch: 80, Loss: 6.839\n",
      "Epoch: 81, Loss: 7.008\n",
      "Epoch: 82, Loss: 6.401\n",
      "Epoch: 83, Loss: 5.705\n",
      "Epoch: 84, Loss: 7.428\n",
      "Epoch: 85, Loss: 5.709\n",
      "Epoch: 86, Loss: 5.740\n",
      "Epoch: 87, Loss: 7.704\n",
      "Epoch: 88, Loss: 5.745\n",
      "Epoch: 89, Loss: 4.771\n",
      "Epoch: 90, Loss: 5.512\n",
      "Epoch: 91, Loss: 6.954\n",
      "Epoch: 92, Loss: 6.559\n",
      "Epoch: 93, Loss: 5.191\n",
      "Epoch: 94, Loss: 5.599\n",
      "Epoch: 95, Loss: 5.297\n",
      "Epoch: 96, Loss: 4.900\n",
      "Epoch: 97, Loss: 6.124\n",
      "Epoch: 98, Loss: 5.959\n",
      "Epoch: 99, Loss: 5.494\n",
      "Epoch: 100, Loss: 5.733\n",
      "Epoch: 101, Loss: 5.413\n",
      "Epoch: 102, Loss: 6.471\n",
      "Epoch: 103, Loss: 4.013\n",
      "Epoch: 104, Loss: 6.198\n",
      "Epoch: 105, Loss: 5.215\n",
      "Epoch: 106, Loss: 5.176\n",
      "Epoch: 107, Loss: 4.509\n",
      "Epoch: 108, Loss: 4.740\n",
      "Epoch: 109, Loss: 6.861\n",
      "Epoch: 110, Loss: 5.337\n",
      "Epoch: 111, Loss: 7.032\n",
      "Epoch: 112, Loss: 4.864\n",
      "Epoch: 113, Loss: 4.951\n",
      "Epoch: 114, Loss: 6.727\n",
      "Epoch: 115, Loss: 6.691\n",
      "Epoch: 116, Loss: 5.560\n",
      "Epoch: 117, Loss: 6.054\n",
      "Epoch: 118, Loss: 5.901\n",
      "Epoch: 119, Loss: 5.356\n",
      "Epoch: 120, Loss: 5.783\n",
      "Epoch: 121, Loss: 4.865\n",
      "Epoch: 122, Loss: 5.804\n",
      "Epoch: 123, Loss: 4.616\n",
      "Epoch: 124, Loss: 4.717\n",
      "Epoch: 125, Loss: 4.803\n",
      "Epoch: 126, Loss: 5.317\n",
      "Epoch: 127, Loss: 6.006\n",
      "Epoch: 128, Loss: 5.087\n",
      "Epoch: 129, Loss: 7.007\n",
      "Epoch: 130, Loss: 5.368\n",
      "Epoch: 131, Loss: 5.714\n",
      "Epoch: 132, Loss: 4.743\n",
      "Epoch: 133, Loss: 5.431\n",
      "Epoch: 134, Loss: 5.876\n",
      "Epoch: 135, Loss: 6.829\n",
      "Epoch: 136, Loss: 6.261\n",
      "Epoch: 137, Loss: 4.636\n",
      "Epoch: 138, Loss: 5.188\n",
      "Epoch: 139, Loss: 5.584\n",
      "Epoch: 140, Loss: 6.219\n",
      "Epoch: 141, Loss: 4.968\n",
      "Epoch: 142, Loss: 5.040\n",
      "Epoch: 143, Loss: 4.473\n",
      "Epoch: 144, Loss: 5.506\n",
      "Epoch: 145, Loss: 4.910\n",
      "Epoch: 146, Loss: 4.996\n",
      "Epoch: 147, Loss: 5.196\n",
      "Epoch: 148, Loss: 5.990\n",
      "Epoch: 149, Loss: 6.224\n",
      "Epoch: 150, Loss: 5.347\n",
      "Epoch: 151, Loss: 5.407\n",
      "Epoch: 152, Loss: 5.135\n",
      "Epoch: 153, Loss: 4.644\n",
      "Epoch: 154, Loss: 4.796\n",
      "Epoch: 155, Loss: 5.104\n",
      "Epoch: 156, Loss: 4.365\n",
      "Epoch: 157, Loss: 5.191\n",
      "Epoch: 158, Loss: 4.429\n",
      "Epoch: 159, Loss: 5.951\n",
      "Epoch: 160, Loss: 4.591\n",
      "Epoch: 161, Loss: 5.170\n",
      "Epoch: 162, Loss: 3.981\n",
      "Epoch: 163, Loss: 4.777\n",
      "Epoch: 164, Loss: 6.770\n",
      "Epoch: 165, Loss: 4.957\n",
      "Epoch: 166, Loss: 5.165\n",
      "Epoch: 167, Loss: 5.319\n",
      "Epoch: 168, Loss: 5.881\n",
      "Epoch: 169, Loss: 3.783\n",
      "Epoch: 170, Loss: 4.259\n",
      "Epoch: 171, Loss: 5.277\n",
      "Epoch: 172, Loss: 4.157\n",
      "Epoch: 173, Loss: 5.093\n",
      "Epoch: 174, Loss: 4.285\n",
      "Epoch: 175, Loss: 4.627\n",
      "Epoch: 176, Loss: 3.968\n",
      "Epoch: 177, Loss: 3.761\n",
      "Epoch: 178, Loss: 4.390\n",
      "Epoch: 179, Loss: 4.541\n",
      "Epoch: 180, Loss: 5.797\n",
      "Epoch: 181, Loss: 5.753\n",
      "Epoch: 182, Loss: 4.513\n",
      "Epoch: 183, Loss: 5.207\n",
      "Epoch: 184, Loss: 4.420\n",
      "Epoch: 185, Loss: 4.564\n",
      "Epoch: 186, Loss: 4.323\n",
      "Epoch: 187, Loss: 6.341\n",
      "Epoch: 188, Loss: 4.205\n",
      "Epoch: 189, Loss: 3.898\n",
      "Epoch: 190, Loss: 6.749\n",
      "Epoch: 191, Loss: 5.215\n",
      "Epoch: 192, Loss: 5.788\n",
      "Epoch: 193, Loss: 4.983\n",
      "Epoch: 194, Loss: 4.377\n",
      "Epoch: 195, Loss: 4.509\n",
      "Epoch: 196, Loss: 4.776\n",
      "Epoch: 197, Loss: 4.504\n",
      "Epoch: 198, Loss: 5.312\n",
      "Epoch: 199, Loss: 4.627\n",
      "Epoch: 200, Loss: 4.084\n",
      "Epoch: 201, Loss: 5.140\n",
      "Epoch: 202, Loss: 4.832\n",
      "Epoch: 203, Loss: 4.080\n",
      "Epoch: 204, Loss: 5.224\n",
      "Epoch: 205, Loss: 5.198\n",
      "Epoch: 206, Loss: 3.806\n",
      "Epoch: 207, Loss: 5.558\n",
      "Epoch: 208, Loss: 4.731\n",
      "Epoch: 209, Loss: 4.114\n",
      "Epoch: 210, Loss: 4.295\n",
      "Epoch: 211, Loss: 4.302\n",
      "Epoch: 212, Loss: 4.336\n",
      "Epoch: 213, Loss: 3.684\n",
      "Epoch: 214, Loss: 5.055\n",
      "Epoch: 215, Loss: 4.406\n",
      "Epoch: 216, Loss: 4.607\n",
      "Epoch: 217, Loss: 5.718\n",
      "Epoch: 218, Loss: 4.370\n",
      "Epoch: 219, Loss: 4.945\n",
      "Epoch: 220, Loss: 5.179\n",
      "Epoch: 221, Loss: 4.528\n",
      "Epoch: 222, Loss: 5.498\n",
      "Epoch: 223, Loss: 4.614\n",
      "Epoch: 224, Loss: 4.428\n",
      "Epoch: 225, Loss: 4.325\n",
      "Epoch: 226, Loss: 4.364\n",
      "Epoch: 227, Loss: 3.993\n",
      "Epoch: 228, Loss: 3.898\n",
      "Epoch: 229, Loss: 4.927\n",
      "Epoch: 230, Loss: 5.239\n",
      "Epoch: 231, Loss: 5.020\n",
      "Epoch: 232, Loss: 4.063\n",
      "Epoch: 233, Loss: 5.261\n",
      "Epoch: 234, Loss: 5.705\n",
      "Epoch: 235, Loss: 4.022\n",
      "Epoch: 236, Loss: 6.268\n",
      "Epoch: 237, Loss: 4.872\n",
      "Epoch: 238, Loss: 5.284\n",
      "Epoch: 239, Loss: 5.496\n",
      "Epoch: 240, Loss: 3.879\n",
      "Epoch: 241, Loss: 3.995\n",
      "Epoch: 242, Loss: 4.037\n",
      "Epoch: 243, Loss: 4.787\n",
      "Epoch: 244, Loss: 3.853\n",
      "Epoch: 245, Loss: 4.589\n",
      "Epoch: 246, Loss: 4.877\n",
      "Epoch: 247, Loss: 4.582\n",
      "Epoch: 248, Loss: 5.146\n",
      "Epoch: 249, Loss: 3.999\n",
      "Epoch: 250, Loss: 4.301\n",
      "Epoch: 251, Loss: 5.705\n",
      "Epoch: 252, Loss: 4.980\n",
      "Epoch: 253, Loss: 4.359\n",
      "Epoch: 254, Loss: 5.458\n",
      "Epoch: 255, Loss: 3.786\n",
      "Epoch: 256, Loss: 4.093\n",
      "Epoch: 257, Loss: 4.142\n",
      "Epoch: 258, Loss: 4.713\n",
      "Epoch: 259, Loss: 4.168\n",
      "Epoch: 260, Loss: 5.019\n",
      "Epoch: 261, Loss: 4.395\n",
      "Epoch: 262, Loss: 5.045\n",
      "Epoch: 263, Loss: 4.225\n",
      "Epoch: 264, Loss: 5.491\n",
      "Epoch: 265, Loss: 5.305\n",
      "Epoch: 266, Loss: 4.849\n",
      "Epoch: 267, Loss: 5.080\n",
      "Epoch: 268, Loss: 4.700\n",
      "Epoch: 269, Loss: 4.002\n",
      "Epoch: 270, Loss: 4.586\n",
      "Epoch: 271, Loss: 4.839\n",
      "Epoch: 272, Loss: 3.569\n",
      "Epoch: 273, Loss: 4.557\n",
      "Epoch: 274, Loss: 4.904\n",
      "Epoch: 275, Loss: 5.853\n",
      "Epoch: 276, Loss: 4.436\n",
      "Epoch: 277, Loss: 3.965\n",
      "Epoch: 278, Loss: 4.364\n",
      "Epoch: 279, Loss: 4.440\n",
      "Epoch: 280, Loss: 4.687\n",
      "Epoch: 281, Loss: 4.522\n",
      "Epoch: 282, Loss: 4.316\n",
      "Epoch: 283, Loss: 5.384\n",
      "Epoch: 284, Loss: 5.037\n",
      "Epoch: 285, Loss: 3.840\n",
      "Epoch: 286, Loss: 4.237\n",
      "Epoch: 287, Loss: 4.807\n",
      "Epoch: 288, Loss: 3.899\n",
      "Epoch: 289, Loss: 4.542\n",
      "Epoch: 290, Loss: 4.678\n",
      "Epoch: 291, Loss: 5.198\n",
      "Epoch: 292, Loss: 4.479\n",
      "Epoch: 293, Loss: 4.915\n",
      "Epoch: 294, Loss: 4.709\n",
      "Epoch: 295, Loss: 4.205\n",
      "Epoch: 296, Loss: 5.329\n",
      "Epoch: 297, Loss: 5.169\n",
      "Epoch: 298, Loss: 3.914\n",
      "Epoch: 299, Loss: 4.918\n",
      "Epoch: 300, Loss: 4.493\n",
      "Epoch: 301, Loss: 5.068\n",
      "Epoch: 302, Loss: 4.364\n",
      "Epoch: 303, Loss: 4.294\n",
      "Epoch: 304, Loss: 4.930\n",
      "Epoch: 305, Loss: 3.987\n",
      "Epoch: 306, Loss: 4.236\n",
      "Epoch: 307, Loss: 3.353\n",
      "Epoch: 308, Loss: 3.303\n",
      "Epoch: 309, Loss: 5.486\n",
      "Epoch: 310, Loss: 4.394\n",
      "Epoch: 311, Loss: 4.018\n",
      "Epoch: 312, Loss: 4.068\n",
      "Epoch: 313, Loss: 4.950\n",
      "Epoch: 314, Loss: 4.142\n",
      "Epoch: 315, Loss: 5.321\n",
      "Epoch: 316, Loss: 5.258\n",
      "Epoch: 317, Loss: 4.781\n",
      "Epoch: 318, Loss: 3.792\n",
      "Epoch: 319, Loss: 3.728\n",
      "Epoch: 320, Loss: 4.682\n",
      "Epoch: 321, Loss: 3.676\n",
      "Epoch: 322, Loss: 4.231\n",
      "Epoch: 323, Loss: 4.829\n",
      "Epoch: 324, Loss: 4.281\n",
      "Epoch: 325, Loss: 3.853\n",
      "Epoch: 326, Loss: 4.719\n",
      "Epoch: 327, Loss: 5.046\n",
      "Epoch: 328, Loss: 3.749\n",
      "Epoch: 329, Loss: 4.943\n",
      "Epoch: 330, Loss: 4.578\n",
      "Epoch: 331, Loss: 4.335\n",
      "Epoch: 332, Loss: 3.865\n",
      "Epoch: 333, Loss: 3.886\n",
      "Epoch: 334, Loss: 4.166\n",
      "Epoch: 335, Loss: 5.311\n",
      "Epoch: 336, Loss: 3.875\n",
      "Epoch: 337, Loss: 3.978\n",
      "Epoch: 338, Loss: 3.424\n",
      "Epoch: 339, Loss: 3.704\n",
      "Epoch: 340, Loss: 4.499\n",
      "Epoch: 341, Loss: 5.296\n",
      "Epoch: 342, Loss: 5.646\n",
      "Epoch: 343, Loss: 4.357\n",
      "Epoch: 344, Loss: 4.490\n",
      "Epoch: 345, Loss: 4.976\n",
      "Epoch: 346, Loss: 3.981\n",
      "Epoch: 347, Loss: 4.677\n",
      "Epoch: 348, Loss: 5.236\n",
      "Epoch: 349, Loss: 4.414\n",
      "Epoch: 350, Loss: 3.991\n",
      "Epoch: 351, Loss: 4.965\n",
      "Epoch: 352, Loss: 4.115\n",
      "Epoch: 353, Loss: 3.800\n",
      "Epoch: 354, Loss: 3.746\n",
      "Epoch: 355, Loss: 3.997\n",
      "Epoch: 356, Loss: 3.662\n",
      "Epoch: 357, Loss: 3.991\n",
      "Epoch: 358, Loss: 4.215\n",
      "Epoch: 359, Loss: 4.298\n",
      "Epoch: 360, Loss: 3.988\n",
      "Epoch: 361, Loss: 4.477\n",
      "Epoch: 362, Loss: 4.715\n",
      "Epoch: 363, Loss: 3.985\n",
      "Epoch: 364, Loss: 4.777\n",
      "Epoch: 365, Loss: 5.418\n",
      "Epoch: 366, Loss: 3.782\n",
      "Epoch: 367, Loss: 4.365\n",
      "Epoch: 368, Loss: 4.661\n",
      "Epoch: 369, Loss: 4.434\n",
      "Epoch: 370, Loss: 4.374\n",
      "Epoch: 371, Loss: 3.921\n",
      "Epoch: 372, Loss: 4.716\n",
      "Epoch: 373, Loss: 3.667\n",
      "Epoch: 374, Loss: 4.676\n",
      "Epoch: 375, Loss: 3.968\n",
      "Epoch: 376, Loss: 4.316\n",
      "Epoch: 377, Loss: 4.715\n",
      "Epoch: 378, Loss: 4.672\n",
      "Epoch: 379, Loss: 4.635\n",
      "Epoch: 380, Loss: 4.684\n",
      "Epoch: 381, Loss: 4.369\n",
      "Epoch: 382, Loss: 4.153\n",
      "Epoch: 383, Loss: 4.996\n",
      "Epoch: 384, Loss: 4.459\n",
      "Epoch: 385, Loss: 3.814\n",
      "Epoch: 386, Loss: 4.307\n",
      "Epoch: 387, Loss: 4.318\n",
      "Epoch: 388, Loss: 3.897\n",
      "Epoch: 389, Loss: 4.737\n",
      "Epoch: 390, Loss: 5.108\n",
      "Epoch: 391, Loss: 4.448\n",
      "Epoch: 392, Loss: 3.811\n",
      "Epoch: 393, Loss: 4.233\n",
      "Epoch: 394, Loss: 4.302\n",
      "Epoch: 395, Loss: 4.273\n",
      "Epoch: 396, Loss: 4.243\n",
      "Epoch: 397, Loss: 4.162\n",
      "Epoch: 398, Loss: 3.988\n",
      "Epoch: 399, Loss: 4.334\n",
      "Epoch: 400, Loss: 4.926\n",
      "Epoch: 401, Loss: 4.271\n",
      "Epoch: 402, Loss: 3.436\n",
      "Epoch: 403, Loss: 4.110\n",
      "Epoch: 404, Loss: 4.447\n",
      "Epoch: 405, Loss: 4.182\n",
      "Epoch: 406, Loss: 3.999\n",
      "Epoch: 407, Loss: 4.208\n",
      "Epoch: 408, Loss: 4.283\n",
      "Epoch: 409, Loss: 4.961\n",
      "Epoch: 410, Loss: 4.032\n",
      "Epoch: 411, Loss: 4.682\n",
      "Epoch: 412, Loss: 5.176\n",
      "Epoch: 413, Loss: 4.676\n",
      "Epoch: 414, Loss: 4.013\n",
      "Epoch: 415, Loss: 3.596\n",
      "Epoch: 416, Loss: 4.979\n",
      "Epoch: 417, Loss: 4.057\n",
      "Epoch: 418, Loss: 4.841\n",
      "Epoch: 419, Loss: 4.918\n",
      "Epoch: 420, Loss: 4.137\n",
      "Epoch: 421, Loss: 4.779\n",
      "Epoch: 422, Loss: 4.912\n",
      "Epoch: 423, Loss: 4.921\n",
      "Epoch: 424, Loss: 4.133\n",
      "Epoch: 425, Loss: 4.458\n",
      "Epoch: 426, Loss: 3.662\n",
      "Epoch: 427, Loss: 3.796\n",
      "Epoch: 428, Loss: 4.392\n",
      "Epoch: 429, Loss: 4.572\n",
      "Epoch: 430, Loss: 3.805\n",
      "Epoch: 431, Loss: 3.378\n",
      "Epoch: 432, Loss: 3.068\n",
      "Epoch: 433, Loss: 5.258\n",
      "Epoch: 434, Loss: 4.350\n",
      "Epoch: 435, Loss: 5.090\n",
      "Epoch: 436, Loss: 4.766\n",
      "Epoch: 437, Loss: 4.536\n",
      "Epoch: 438, Loss: 4.436\n",
      "Epoch: 439, Loss: 3.922\n",
      "Epoch: 440, Loss: 4.125\n",
      "Epoch: 441, Loss: 4.148\n",
      "Epoch: 442, Loss: 3.956\n",
      "Epoch: 443, Loss: 3.927\n",
      "Epoch: 444, Loss: 5.341\n",
      "Epoch: 445, Loss: 4.702\n",
      "Epoch: 446, Loss: 4.448\n",
      "Epoch: 447, Loss: 3.751\n",
      "Epoch: 448, Loss: 3.623\n",
      "Epoch: 449, Loss: 4.869\n",
      "Epoch: 450, Loss: 4.185\n",
      "Epoch: 451, Loss: 4.502\n",
      "Epoch: 452, Loss: 3.807\n",
      "Epoch: 453, Loss: 4.690\n",
      "Epoch: 454, Loss: 4.790\n",
      "Epoch: 455, Loss: 4.150\n",
      "Epoch: 456, Loss: 4.545\n",
      "Epoch: 457, Loss: 3.462\n",
      "Epoch: 458, Loss: 3.411\n",
      "Epoch: 459, Loss: 5.211\n",
      "Epoch: 460, Loss: 3.681\n",
      "Epoch: 461, Loss: 5.036\n",
      "Epoch: 462, Loss: 4.233\n",
      "Epoch: 463, Loss: 4.115\n",
      "Epoch: 464, Loss: 3.560\n",
      "Epoch: 465, Loss: 4.993\n",
      "Epoch: 466, Loss: 4.595\n",
      "Epoch: 467, Loss: 3.419\n",
      "Epoch: 468, Loss: 5.233\n",
      "Epoch: 469, Loss: 4.257\n",
      "Epoch: 470, Loss: 4.049\n",
      "Epoch: 471, Loss: 3.822\n",
      "Epoch: 472, Loss: 3.919\n",
      "Epoch: 473, Loss: 3.734\n",
      "Epoch: 474, Loss: 3.655\n",
      "Epoch: 475, Loss: 3.677\n",
      "Epoch: 476, Loss: 4.646\n",
      "Epoch: 477, Loss: 3.632\n",
      "Epoch: 478, Loss: 4.570\n",
      "Epoch: 479, Loss: 4.552\n",
      "Epoch: 480, Loss: 4.541\n",
      "Epoch: 481, Loss: 4.155\n",
      "Epoch: 482, Loss: 6.289\n",
      "Epoch: 483, Loss: 4.608\n",
      "Epoch: 484, Loss: 3.505\n",
      "Epoch: 485, Loss: 4.015\n",
      "Epoch: 486, Loss: 3.770\n",
      "Epoch: 487, Loss: 4.540\n",
      "Epoch: 488, Loss: 4.174\n",
      "Epoch: 489, Loss: 4.022\n",
      "Epoch: 490, Loss: 3.699\n",
      "Epoch: 491, Loss: 6.063\n",
      "Epoch: 492, Loss: 3.575\n",
      "Epoch: 493, Loss: 4.061\n",
      "Epoch: 494, Loss: 3.436\n",
      "Epoch: 495, Loss: 4.764\n",
      "Epoch: 496, Loss: 4.036\n",
      "Epoch: 497, Loss: 4.145\n",
      "Epoch: 498, Loss: 3.805\n",
      "Epoch: 499, Loss: 4.198\n",
      "Epoch: 500, Loss: 3.297\n",
      "Epoch: 501, Loss: 4.316\n",
      "Epoch: 502, Loss: 4.397\n",
      "Epoch: 503, Loss: 4.619\n",
      "Epoch: 504, Loss: 3.970\n",
      "Epoch: 505, Loss: 4.116\n",
      "Epoch: 506, Loss: 4.164\n",
      "Epoch: 507, Loss: 4.619\n",
      "Epoch: 508, Loss: 3.702\n",
      "Epoch: 509, Loss: 3.734\n",
      "Epoch: 510, Loss: 4.809\n",
      "Epoch: 511, Loss: 3.676\n",
      "Epoch: 512, Loss: 5.335\n",
      "Epoch: 513, Loss: 3.750\n",
      "Epoch: 514, Loss: 5.025\n",
      "Epoch: 515, Loss: 4.561\n",
      "Epoch: 516, Loss: 4.478\n",
      "Epoch: 517, Loss: 3.863\n",
      "Epoch: 518, Loss: 4.634\n",
      "Epoch: 519, Loss: 3.886\n",
      "Epoch: 520, Loss: 4.358\n",
      "Epoch: 521, Loss: 3.473\n",
      "Epoch: 522, Loss: 4.186\n",
      "Epoch: 523, Loss: 4.162\n",
      "Epoch: 524, Loss: 4.015\n",
      "Epoch: 525, Loss: 4.286\n",
      "Epoch: 526, Loss: 4.686\n",
      "Epoch: 527, Loss: 3.311\n",
      "Epoch: 528, Loss: 3.446\n",
      "Epoch: 529, Loss: 3.682\n",
      "Epoch: 530, Loss: 3.841\n",
      "Epoch: 531, Loss: 4.343\n",
      "Epoch: 532, Loss: 3.872\n",
      "Epoch: 533, Loss: 3.740\n",
      "Epoch: 534, Loss: 4.443\n",
      "Epoch: 535, Loss: 5.309\n",
      "Epoch: 536, Loss: 4.916\n",
      "Epoch: 537, Loss: 4.666\n",
      "Epoch: 538, Loss: 4.792\n",
      "Epoch: 539, Loss: 3.822\n",
      "Epoch: 540, Loss: 3.825\n",
      "Epoch: 541, Loss: 3.601\n",
      "Epoch: 542, Loss: 3.125\n",
      "Epoch: 543, Loss: 4.584\n",
      "Epoch: 544, Loss: 4.090\n",
      "Epoch: 545, Loss: 4.283\n",
      "Epoch: 546, Loss: 4.984\n",
      "Epoch: 547, Loss: 3.212\n",
      "Epoch: 548, Loss: 4.632\n",
      "Epoch: 549, Loss: 4.691\n",
      "Epoch: 550, Loss: 4.469\n",
      "Epoch: 551, Loss: 3.833\n",
      "Epoch: 552, Loss: 4.111\n",
      "Epoch: 553, Loss: 4.721\n",
      "Epoch: 554, Loss: 4.809\n",
      "Epoch: 555, Loss: 4.493\n",
      "Epoch: 556, Loss: 4.254\n",
      "Epoch: 557, Loss: 4.590\n",
      "Epoch: 558, Loss: 4.054\n",
      "Epoch: 559, Loss: 4.646\n",
      "Epoch: 560, Loss: 3.792\n",
      "Epoch: 561, Loss: 3.716\n",
      "Epoch: 562, Loss: 3.648\n",
      "Epoch: 563, Loss: 3.807\n",
      "Epoch: 564, Loss: 3.658\n",
      "Epoch: 565, Loss: 3.383\n",
      "Epoch: 566, Loss: 4.508\n",
      "Epoch: 567, Loss: 5.378\n",
      "Epoch: 568, Loss: 4.634\n",
      "Epoch: 569, Loss: 4.965\n",
      "Epoch: 570, Loss: 4.406\n",
      "Epoch: 571, Loss: 4.148\n",
      "Epoch: 572, Loss: 3.775\n",
      "Epoch: 573, Loss: 3.962\n",
      "Epoch: 574, Loss: 4.310\n",
      "Epoch: 575, Loss: 4.427\n",
      "Epoch: 576, Loss: 3.957\n",
      "Epoch: 577, Loss: 4.614\n",
      "Epoch: 578, Loss: 4.323\n",
      "Epoch: 579, Loss: 4.095\n",
      "Epoch: 580, Loss: 4.846\n",
      "Epoch: 581, Loss: 4.644\n",
      "Epoch: 582, Loss: 3.811\n",
      "Epoch: 583, Loss: 3.889\n",
      "Epoch: 584, Loss: 4.784\n",
      "Epoch: 585, Loss: 4.592\n",
      "Epoch: 586, Loss: 4.533\n",
      "Epoch: 587, Loss: 4.215\n",
      "Epoch: 588, Loss: 4.462\n",
      "Epoch: 589, Loss: 4.519\n",
      "Epoch: 590, Loss: 4.726\n",
      "Epoch: 591, Loss: 3.693\n",
      "Epoch: 592, Loss: 4.094\n",
      "Epoch: 593, Loss: 4.002\n",
      "Epoch: 594, Loss: 4.323\n",
      "Epoch: 595, Loss: 4.645\n",
      "Epoch: 596, Loss: 4.014\n",
      "Epoch: 597, Loss: 3.516\n",
      "Epoch: 598, Loss: 4.320\n",
      "Epoch: 599, Loss: 4.417\n",
      "Epoch: 600, Loss: 4.929\n",
      "Epoch: 601, Loss: 4.383\n",
      "Epoch: 602, Loss: 3.404\n",
      "Epoch: 603, Loss: 3.831\n",
      "Epoch: 604, Loss: 4.847\n",
      "Epoch: 605, Loss: 3.589\n",
      "Epoch: 606, Loss: 4.386\n",
      "Epoch: 607, Loss: 4.570\n",
      "Epoch: 608, Loss: 4.957\n",
      "Epoch: 609, Loss: 4.196\n",
      "Epoch: 610, Loss: 4.490\n",
      "Epoch: 611, Loss: 4.132\n",
      "Epoch: 612, Loss: 4.797\n",
      "Epoch: 613, Loss: 3.997\n",
      "Epoch: 614, Loss: 3.881\n",
      "Epoch: 615, Loss: 3.485\n",
      "Epoch: 616, Loss: 3.897\n",
      "Epoch: 617, Loss: 3.521\n",
      "Epoch: 618, Loss: 4.986\n",
      "Epoch: 619, Loss: 4.166\n",
      "Epoch: 620, Loss: 4.677\n",
      "Epoch: 621, Loss: 4.068\n",
      "Epoch: 622, Loss: 4.146\n",
      "Epoch: 623, Loss: 4.130\n",
      "Epoch: 624, Loss: 5.062\n",
      "Epoch: 625, Loss: 3.929\n",
      "Epoch: 626, Loss: 4.424\n",
      "Epoch: 627, Loss: 4.426\n",
      "Epoch: 628, Loss: 3.939\n",
      "Epoch: 629, Loss: 4.265\n",
      "Epoch: 630, Loss: 5.500\n",
      "Epoch: 631, Loss: 4.366\n",
      "Epoch: 632, Loss: 4.725\n",
      "Epoch: 633, Loss: 3.958\n",
      "Epoch: 634, Loss: 3.914\n",
      "Epoch: 635, Loss: 4.632\n",
      "Epoch: 636, Loss: 4.417\n",
      "Epoch: 637, Loss: 4.454\n",
      "Epoch: 638, Loss: 3.591\n",
      "Epoch: 639, Loss: 4.066\n",
      "Epoch: 640, Loss: 4.078\n",
      "Epoch: 641, Loss: 4.369\n",
      "Epoch: 642, Loss: 4.309\n",
      "Epoch: 643, Loss: 4.129\n",
      "Epoch: 644, Loss: 4.147\n",
      "Epoch: 645, Loss: 4.404\n",
      "Epoch: 646, Loss: 4.265\n",
      "Epoch: 647, Loss: 3.121\n",
      "Epoch: 648, Loss: 3.869\n",
      "Epoch: 649, Loss: 3.476\n",
      "Epoch: 650, Loss: 3.740\n",
      "Epoch: 651, Loss: 3.162\n",
      "Epoch: 652, Loss: 3.464\n",
      "Epoch: 653, Loss: 3.569\n",
      "Epoch: 654, Loss: 4.671\n",
      "Epoch: 655, Loss: 4.321\n",
      "Epoch: 656, Loss: 4.586\n",
      "Epoch: 657, Loss: 4.578\n",
      "Epoch: 658, Loss: 3.485\n",
      "Epoch: 659, Loss: 3.882\n",
      "Epoch: 660, Loss: 5.392\n",
      "Epoch: 661, Loss: 3.760\n",
      "Epoch: 662, Loss: 4.607\n",
      "Epoch: 663, Loss: 4.175\n",
      "Epoch: 664, Loss: 3.410\n",
      "Epoch: 665, Loss: 3.917\n",
      "Epoch: 666, Loss: 3.771\n",
      "Epoch: 667, Loss: 4.317\n",
      "Epoch: 668, Loss: 4.140\n",
      "Epoch: 669, Loss: 4.031\n",
      "Epoch: 670, Loss: 3.388\n",
      "Epoch: 671, Loss: 4.606\n",
      "Epoch: 672, Loss: 3.711\n",
      "Epoch: 673, Loss: 4.532\n",
      "Epoch: 674, Loss: 4.058\n",
      "Epoch: 675, Loss: 3.716\n",
      "Epoch: 676, Loss: 3.597\n",
      "Epoch: 677, Loss: 4.449\n",
      "Epoch: 678, Loss: 4.534\n",
      "Epoch: 679, Loss: 4.530\n",
      "Epoch: 680, Loss: 4.652\n",
      "Epoch: 681, Loss: 3.634\n",
      "Epoch: 682, Loss: 3.589\n",
      "Epoch: 683, Loss: 4.395\n",
      "Epoch: 684, Loss: 5.551\n",
      "Epoch: 685, Loss: 3.794\n",
      "Epoch: 686, Loss: 4.225\n",
      "Epoch: 687, Loss: 3.785\n",
      "Epoch: 688, Loss: 3.524\n",
      "Epoch: 689, Loss: 4.185\n",
      "Epoch: 690, Loss: 4.737\n",
      "Epoch: 691, Loss: 2.897\n",
      "Epoch: 692, Loss: 4.089\n",
      "Epoch: 693, Loss: 4.674\n",
      "Epoch: 694, Loss: 3.775\n",
      "Epoch: 695, Loss: 3.844\n",
      "Epoch: 696, Loss: 3.404\n",
      "Epoch: 697, Loss: 4.348\n",
      "Epoch: 698, Loss: 3.331\n",
      "Epoch: 699, Loss: 3.575\n",
      "Epoch: 700, Loss: 3.601\n",
      "Epoch: 701, Loss: 4.245\n",
      "Epoch: 702, Loss: 3.807\n",
      "Epoch: 703, Loss: 3.281\n",
      "Epoch: 704, Loss: 5.244\n",
      "Epoch: 705, Loss: 4.199\n",
      "Epoch: 706, Loss: 4.095\n",
      "Epoch: 707, Loss: 3.703\n",
      "Epoch: 708, Loss: 3.569\n",
      "Epoch: 709, Loss: 4.650\n",
      "Epoch: 710, Loss: 5.111\n",
      "Epoch: 711, Loss: 4.276\n",
      "Epoch: 712, Loss: 3.702\n",
      "Epoch: 713, Loss: 3.757\n",
      "Epoch: 714, Loss: 4.130\n",
      "Epoch: 715, Loss: 4.278\n",
      "Epoch: 716, Loss: 4.775\n",
      "Epoch: 717, Loss: 4.365\n",
      "Epoch: 718, Loss: 3.908\n",
      "Epoch: 719, Loss: 4.820\n",
      "Epoch: 720, Loss: 3.473\n",
      "Epoch: 721, Loss: 5.179\n",
      "Epoch: 722, Loss: 4.069\n",
      "Epoch: 723, Loss: 4.316\n",
      "Epoch: 724, Loss: 3.074\n",
      "Epoch: 725, Loss: 4.632\n",
      "Epoch: 726, Loss: 4.177\n",
      "Epoch: 727, Loss: 3.326\n",
      "Epoch: 728, Loss: 4.089\n",
      "Epoch: 729, Loss: 3.861\n",
      "Epoch: 730, Loss: 3.529\n",
      "Epoch: 731, Loss: 3.536\n",
      "Epoch: 732, Loss: 4.820\n",
      "Epoch: 733, Loss: 3.354\n",
      "Epoch: 734, Loss: 3.492\n",
      "Epoch: 735, Loss: 4.153\n",
      "Epoch: 736, Loss: 4.249\n",
      "Epoch: 737, Loss: 4.952\n",
      "Epoch: 738, Loss: 3.651\n",
      "Epoch: 739, Loss: 3.469\n",
      "Epoch: 740, Loss: 4.220\n",
      "Epoch: 741, Loss: 3.886\n",
      "Epoch: 742, Loss: 4.323\n",
      "Epoch: 743, Loss: 3.440\n",
      "Epoch: 744, Loss: 4.610\n",
      "Epoch: 745, Loss: 3.515\n",
      "Epoch: 746, Loss: 3.934\n",
      "Epoch: 747, Loss: 3.619\n",
      "Epoch: 748, Loss: 3.514\n",
      "Epoch: 749, Loss: 3.817\n",
      "Epoch: 750, Loss: 4.788\n",
      "Epoch: 751, Loss: 4.508\n",
      "Epoch: 752, Loss: 3.859\n",
      "Epoch: 753, Loss: 4.364\n",
      "Epoch: 754, Loss: 3.487\n",
      "Epoch: 755, Loss: 3.719\n",
      "Epoch: 756, Loss: 3.511\n",
      "Epoch: 757, Loss: 3.893\n",
      "Epoch: 758, Loss: 4.340\n",
      "Epoch: 759, Loss: 3.794\n",
      "Epoch: 760, Loss: 3.541\n",
      "Epoch: 761, Loss: 4.567\n",
      "Epoch: 762, Loss: 3.544\n",
      "Epoch: 763, Loss: 4.329\n",
      "Epoch: 764, Loss: 5.177\n",
      "Epoch: 765, Loss: 3.354\n",
      "Epoch: 766, Loss: 4.410\n",
      "Epoch: 767, Loss: 4.674\n",
      "Epoch: 768, Loss: 3.526\n",
      "Epoch: 769, Loss: 4.335\n",
      "Epoch: 770, Loss: 4.000\n",
      "Epoch: 771, Loss: 4.116\n",
      "Epoch: 772, Loss: 3.910\n",
      "Epoch: 773, Loss: 4.382\n",
      "Epoch: 774, Loss: 4.573\n",
      "Epoch: 775, Loss: 4.544\n",
      "Epoch: 776, Loss: 3.569\n",
      "Epoch: 777, Loss: 3.607\n",
      "Epoch: 778, Loss: 3.750\n",
      "Epoch: 779, Loss: 4.130\n",
      "Epoch: 780, Loss: 3.882\n",
      "Epoch: 781, Loss: 4.123\n",
      "Epoch: 782, Loss: 3.597\n",
      "Epoch: 783, Loss: 4.328\n",
      "Epoch: 784, Loss: 3.668\n",
      "Epoch: 785, Loss: 4.684\n",
      "Epoch: 786, Loss: 4.723\n",
      "Epoch: 787, Loss: 3.688\n",
      "Epoch: 788, Loss: 3.695\n",
      "Epoch: 789, Loss: 3.925\n",
      "Epoch: 790, Loss: 3.831\n",
      "Epoch: 791, Loss: 3.896\n",
      "Epoch: 792, Loss: 4.672\n",
      "Epoch: 793, Loss: 3.894\n",
      "Epoch: 794, Loss: 3.315\n",
      "Epoch: 795, Loss: 4.969\n",
      "Epoch: 796, Loss: 4.361\n",
      "Epoch: 797, Loss: 3.901\n",
      "Epoch: 798, Loss: 4.617\n",
      "Epoch: 799, Loss: 4.074\n",
      "Epoch: 800, Loss: 4.091\n",
      "Epoch: 801, Loss: 3.943\n",
      "Epoch: 802, Loss: 4.719\n",
      "Epoch: 803, Loss: 4.151\n",
      "Epoch: 804, Loss: 3.563\n",
      "Epoch: 805, Loss: 4.085\n",
      "Epoch: 806, Loss: 4.311\n",
      "Epoch: 807, Loss: 3.592\n",
      "Epoch: 808, Loss: 4.418\n",
      "Epoch: 809, Loss: 4.792\n",
      "Epoch: 810, Loss: 3.511\n",
      "Epoch: 811, Loss: 4.127\n",
      "Epoch: 812, Loss: 3.992\n",
      "Epoch: 813, Loss: 4.262\n",
      "Epoch: 814, Loss: 4.304\n",
      "Epoch: 815, Loss: 3.725\n",
      "Epoch: 816, Loss: 4.112\n",
      "Epoch: 817, Loss: 4.156\n",
      "Epoch: 818, Loss: 3.855\n",
      "Epoch: 819, Loss: 4.333\n",
      "Epoch: 820, Loss: 4.483\n",
      "Epoch: 821, Loss: 3.848\n",
      "Epoch: 822, Loss: 4.167\n",
      "Epoch: 823, Loss: 4.051\n",
      "Epoch: 824, Loss: 4.492\n",
      "Epoch: 825, Loss: 4.397\n",
      "Epoch: 826, Loss: 3.647\n",
      "Epoch: 827, Loss: 4.285\n",
      "Epoch: 828, Loss: 3.683\n",
      "Epoch: 829, Loss: 4.217\n",
      "Epoch: 830, Loss: 3.446\n",
      "Epoch: 831, Loss: 4.684\n",
      "Epoch: 832, Loss: 3.337\n",
      "Epoch: 833, Loss: 3.800\n",
      "Epoch: 834, Loss: 3.873\n",
      "Epoch: 835, Loss: 4.131\n",
      "Epoch: 836, Loss: 4.012\n",
      "Epoch: 837, Loss: 3.846\n",
      "Epoch: 838, Loss: 4.077\n",
      "Epoch: 839, Loss: 3.492\n",
      "Epoch: 840, Loss: 4.138\n",
      "Epoch: 841, Loss: 4.477\n",
      "Epoch: 842, Loss: 3.972\n",
      "Epoch: 843, Loss: 4.363\n",
      "Epoch: 844, Loss: 4.304\n",
      "Epoch: 845, Loss: 3.429\n",
      "Epoch: 846, Loss: 3.811\n",
      "Epoch: 847, Loss: 3.752\n",
      "Epoch: 848, Loss: 3.466\n",
      "Epoch: 849, Loss: 3.595\n",
      "Epoch: 850, Loss: 4.114\n",
      "Epoch: 851, Loss: 4.014\n",
      "Epoch: 852, Loss: 3.649\n",
      "Epoch: 853, Loss: 3.683\n",
      "Epoch: 854, Loss: 3.470\n",
      "Epoch: 855, Loss: 3.388\n",
      "Epoch: 856, Loss: 3.630\n",
      "Epoch: 857, Loss: 4.102\n",
      "Epoch: 858, Loss: 4.084\n",
      "Epoch: 859, Loss: 3.405\n",
      "Epoch: 860, Loss: 4.362\n",
      "Epoch: 861, Loss: 3.842\n",
      "Epoch: 862, Loss: 4.285\n",
      "Epoch: 863, Loss: 4.939\n",
      "Epoch: 864, Loss: 4.606\n",
      "Epoch: 865, Loss: 3.506\n",
      "Epoch: 866, Loss: 3.750\n",
      "Epoch: 867, Loss: 4.397\n",
      "Epoch: 868, Loss: 4.134\n",
      "Epoch: 869, Loss: 3.860\n",
      "Epoch: 870, Loss: 3.619\n",
      "Epoch: 871, Loss: 4.223\n",
      "Epoch: 872, Loss: 3.485\n",
      "Epoch: 873, Loss: 4.465\n",
      "Epoch: 874, Loss: 3.382\n",
      "Epoch: 875, Loss: 4.007\n",
      "Epoch: 876, Loss: 3.910\n",
      "Epoch: 877, Loss: 4.523\n",
      "Epoch: 878, Loss: 3.968\n",
      "Epoch: 879, Loss: 3.588\n",
      "Epoch: 880, Loss: 3.682\n",
      "Epoch: 881, Loss: 4.307\n",
      "Epoch: 882, Loss: 3.808\n",
      "Epoch: 883, Loss: 4.210\n",
      "Epoch: 884, Loss: 4.647\n",
      "Epoch: 885, Loss: 4.335\n",
      "Epoch: 886, Loss: 4.334\n",
      "Epoch: 887, Loss: 3.429\n",
      "Epoch: 888, Loss: 3.778\n",
      "Epoch: 889, Loss: 3.152\n",
      "Epoch: 890, Loss: 4.019\n",
      "Epoch: 891, Loss: 4.145\n",
      "Epoch: 892, Loss: 3.793\n",
      "Epoch: 893, Loss: 3.475\n",
      "Epoch: 894, Loss: 4.002\n",
      "Epoch: 895, Loss: 3.473\n",
      "Epoch: 896, Loss: 3.729\n",
      "Epoch: 897, Loss: 3.831\n",
      "Epoch: 898, Loss: 3.095\n",
      "Epoch: 899, Loss: 3.228\n",
      "Epoch: 900, Loss: 4.176\n",
      "Epoch: 901, Loss: 3.700\n",
      "Epoch: 902, Loss: 4.085\n",
      "Epoch: 903, Loss: 3.900\n",
      "Epoch: 904, Loss: 3.891\n",
      "Epoch: 905, Loss: 3.940\n",
      "Epoch: 906, Loss: 3.950\n",
      "Epoch: 907, Loss: 3.524\n",
      "Epoch: 908, Loss: 4.147\n",
      "Epoch: 909, Loss: 4.313\n",
      "Epoch: 910, Loss: 4.450\n",
      "Epoch: 911, Loss: 3.850\n",
      "Epoch: 912, Loss: 4.047\n",
      "Epoch: 913, Loss: 3.515\n",
      "Epoch: 914, Loss: 3.917\n",
      "Epoch: 915, Loss: 4.295\n",
      "Epoch: 916, Loss: 4.349\n",
      "Epoch: 917, Loss: 3.960\n",
      "Epoch: 918, Loss: 3.867\n",
      "Epoch: 919, Loss: 4.568\n",
      "Epoch: 920, Loss: 3.668\n",
      "Epoch: 921, Loss: 3.709\n",
      "Epoch: 922, Loss: 3.747\n",
      "Epoch: 923, Loss: 4.432\n",
      "Epoch: 924, Loss: 3.764\n",
      "Epoch: 925, Loss: 3.440\n",
      "Epoch: 926, Loss: 3.507\n",
      "Epoch: 927, Loss: 4.415\n",
      "Epoch: 928, Loss: 3.491\n",
      "Epoch: 929, Loss: 3.619\n",
      "Epoch: 930, Loss: 3.655\n",
      "Epoch: 931, Loss: 3.430\n",
      "Epoch: 932, Loss: 3.922\n",
      "Epoch: 933, Loss: 3.601\n",
      "Epoch: 934, Loss: 3.716\n",
      "Epoch: 935, Loss: 4.182\n",
      "Epoch: 936, Loss: 5.033\n",
      "Epoch: 937, Loss: 3.982\n",
      "Epoch: 938, Loss: 3.892\n",
      "Epoch: 939, Loss: 4.075\n",
      "Epoch: 940, Loss: 3.704\n",
      "Epoch: 941, Loss: 3.683\n",
      "Epoch: 942, Loss: 4.642\n",
      "Epoch: 943, Loss: 3.533\n",
      "Epoch: 944, Loss: 3.236\n",
      "Epoch: 945, Loss: 3.964\n",
      "Epoch: 946, Loss: 3.547\n",
      "Epoch: 947, Loss: 4.346\n",
      "Epoch: 948, Loss: 4.590\n",
      "Epoch: 949, Loss: 3.574\n",
      "Epoch: 950, Loss: 3.394\n",
      "Epoch: 951, Loss: 3.277\n",
      "Epoch: 952, Loss: 3.533\n",
      "Epoch: 953, Loss: 3.594\n",
      "Epoch: 954, Loss: 4.196\n",
      "Epoch: 955, Loss: 4.493\n",
      "Epoch: 956, Loss: 3.410\n",
      "Epoch: 957, Loss: 3.694\n",
      "Epoch: 958, Loss: 3.841\n",
      "Epoch: 959, Loss: 4.283\n",
      "Epoch: 960, Loss: 4.187\n",
      "Epoch: 961, Loss: 4.135\n",
      "Epoch: 962, Loss: 4.344\n",
      "Epoch: 963, Loss: 3.679\n",
      "Epoch: 964, Loss: 4.731\n",
      "Epoch: 965, Loss: 4.664\n",
      "Epoch: 966, Loss: 3.662\n",
      "Epoch: 967, Loss: 3.958\n",
      "Epoch: 968, Loss: 4.715\n",
      "Epoch: 969, Loss: 4.113\n",
      "Epoch: 970, Loss: 3.867\n",
      "Epoch: 971, Loss: 3.938\n",
      "Epoch: 972, Loss: 4.120\n",
      "Epoch: 973, Loss: 3.435\n",
      "Epoch: 974, Loss: 4.471\n",
      "Epoch: 975, Loss: 3.200\n",
      "Epoch: 976, Loss: 4.645\n",
      "Epoch: 977, Loss: 3.594\n",
      "Epoch: 978, Loss: 3.430\n",
      "Epoch: 979, Loss: 3.796\n",
      "Epoch: 980, Loss: 3.564\n",
      "Epoch: 981, Loss: 3.257\n",
      "Epoch: 982, Loss: 3.785\n",
      "Epoch: 983, Loss: 3.471\n",
      "Epoch: 984, Loss: 4.065\n",
      "Epoch: 985, Loss: 4.126\n",
      "Epoch: 986, Loss: 4.196\n",
      "Epoch: 987, Loss: 4.027\n",
      "Epoch: 988, Loss: 3.710\n",
      "Epoch: 989, Loss: 3.907\n",
      "Epoch: 990, Loss: 3.868\n",
      "Epoch: 991, Loss: 3.898\n",
      "Epoch: 992, Loss: 3.412\n",
      "Epoch: 993, Loss: 4.835\n",
      "Epoch: 994, Loss: 4.301\n",
      "Epoch: 995, Loss: 3.857\n",
      "Epoch: 996, Loss: 3.185\n",
      "Epoch: 997, Loss: 3.637\n",
      "Epoch: 998, Loss: 3.283\n",
      "Epoch: 999, Loss: 4.004\n",
      "Epoch: 1000, Loss: 4.117\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 1000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
